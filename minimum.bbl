\begin{thebibliography}{}

\bibitem[Allcott and Gentzkow, 2017]{10.1257/jep.31.2.211}
Allcott, H. and Gentzkow, M. (2017).
\newblock Social media and fake news in the 2016 election.
\newblock {\em Journal of Economic Perspectives}, 31(2):211--36.

\bibitem[Aly et~al., 2021]{aly2021feverous}
Aly, R., Guo, Z., Schlichtkrull, M.~S., Thorne, J., Vlachos, A.,
  Christodoulopoulos, C., Cocarascu, O., and Mittal, A. (2021).
\newblock {FEVEROUS}: Fact extraction and {VER}ification over unstructured and
  structured information.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}.

\bibitem[Banerjee and Lavie, 2005]{banerjee-lavie-2005-meteor}
Banerjee, S. and Lavie, A. (2005).
\newblock {METEOR}: An automatic metric for {MT} evaluation with improved
  correlation with human judgments.
\newblock In {\em Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic
  Evaluation Measures for Machine Translation and/or Summarization}, pages
  65--72, Ann Arbor, Michigan. Association for Computational Linguistics.

\bibitem[Barua et~al., 2020]{BARUA2020100119}
Barua, Z., Barua, S., Aktar, S., Kabir, N., and Li, M. (2020).
\newblock Effects of misinformation on covid-19 individual responses and
  recommendations for resilience of disastrous consequences of misinformation.
\newblock {\em Progress in Disaster Science}, 8:100119.

\bibitem[Brown et~al., 2020]{gpt3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D. (2020).
\newblock Language models are few-shot learners.
\newblock {\em CoRR}, abs/2005.14165.

\bibitem[Buchanan and Benson, 2019]{doi:10.1177/2056305119888654}
Buchanan, T. and Benson, V. (2019).
\newblock Spreading disinformation on facebook: Do trust in message source,
  risk propensity, or personality affect the organic reach of “fake news”?
\newblock {\em Social Media + Society}, 5(4):2056305119888654.

\bibitem[Bútora, 2023]{butora}
Bútora, R. (2023).
\newblock Crowd-sourcing platform frontend for fact-checking.
\newblock \url{https://dspace.cvut.cz/handle/10467/109505}.

\bibitem[Chen et~al., 2017]{drqa}
Chen, D., Fisch, A., Weston, J., and Bordes, A. (2017).
\newblock Reading wikipedia to answer open-domain questions.
\newblock {\em CoRR}, abs/1704.00051.

\bibitem[Chen et~al., 2023]{bing}
Chen, J., Kim, G., Sriram, A., Durrett, G., and Choi, E. (2023).
\newblock Complex claim verification with evidence retrieved in the wild.

\bibitem[Cheng et~al., 2016]{lstm}
Cheng, J., Dong, L., and Lapata, M. (2016).
\newblock Long short-term memory-networks for machine reading.
\newblock {\em CoRR}, abs/1601.06733.

\bibitem[Choi et~al., 2021]{choi-etal-2021-decontextualization}
Choi, E., Palomaki, J., Lamm, M., Kwiatkowski, T., Das, D., and Collins, M.
  (2021).
\newblock Decontextualization: Making sentences stand-alone.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:447--461.

\bibitem[Conneau et~al., 2019]{xlm-roberta}
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G.,
  Guzm{\'{a}}n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V.
  (2019).
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock {\em CoRR}, abs/1911.02116.

\bibitem[Dettmers et~al., 2023]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023).
\newblock Qlora: Efficient finetuning of quantized llms.

\bibitem[Devlin et~al., 2019]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[Elsayed et~al., 2021]{clef19}
Elsayed, T., Nakov, P., Barrón-Cedeño, A., Hasanain, M., Suwaileh, R.,
  Martino, G. D.~S., and Atanasova, P. (2021).
\newblock Overview of the clef-2019 checkthat!: Automatic identification and
  verification of claims.

\bibitem[Fleiss, 1971]{fleiss1971measuring}
Fleiss, J.~L. (1971).
\newblock Measuring nominal scale agreement among many raters.
\newblock {\em Psychological Bulletin}, 76(5):378--382.

\bibitem[Fu et~al., 2023]{fu2023gptscore}
Fu, J., Ng, S.-K., Jiang, Z., and Liu, P. (2023).
\newblock Gptscore: Evaluate as you desire.

\bibitem[Gažo, 2021]{gazo}
Gažo, A. (2021).
\newblock Algorithms for document retrieval in czech language supporting long
  inputs.

\bibitem[Guo et~al., 2022]{guo-etal-2022-survey}
Guo, Z., Schlichtkrull, M., and Vlachos, A. (2022).
\newblock A survey on automated fact-checking.
\newblock {\em Transactions of the Association for Computational Linguistics},
  10:178--206.

\bibitem[Hanselowski et~al., 2018]{fnc}
Hanselowski, A., PVS, A., Schiller, B., Caspelherr, F., Chaudhuri, D., Meyer,
  C.~M., and Gurevych, I. (2018).
\newblock A retrospective analysis of the fake news challenge stance-detection
  task.
\newblock In {\em Proceedings of the 27th International Conference on
  Computational Linguistics}, pages 1859--1874, Santa Fe, New Mexico, USA.
  Association for Computational Linguistics.

\bibitem[Hasan et~al., 2021]{xlsum}
Hasan, T., Bhattacharjee, A., Islam, M.~S., Samin, K., Li, Y., Kang, Y.,
  Rahman, M.~S., and Shahriyar, R. (2021).
\newblock Xl-sum: Large-scale multilingual abstractive summarization for 44
  languages.
\newblock {\em CoRR}, abs/2106.13822.

\bibitem[Hayes and Krippendorff, 2007]{hayes2007krippendorff}
Hayes, A. and Krippendorff, K. (2007).
\newblock Answering the call for a standard reliability measure for coding
  data.
\newblock {\em Communication Methods and Measures}, 1:77--89.

\bibitem[Hu et~al., 2021]{lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen{-}Zhu, Z., Li, Y., Wang, S., and Chen,
  W. (2021).
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em CoRR}, abs/2106.09685.

\bibitem[Ji et~al., 2023]{Ji_2023}
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.~J.,
  Madotto, A., and Fung, P. (2023).
\newblock Survey of hallucination in natural language generation.
\newblock {\em {ACM} Computing Surveys}, 55(12):1--38.

\bibitem[Kocián et~al., 2021]{kocian2021siamese}
Kocián, M., Náplava, J., Štancl, D., and Kadlec, V. (2021).
\newblock Siamese bert-based model for web search relevance ranking evaluated
  on a new czech dataset.

\bibitem[Koto et~al., 2020]{ffci}
Koto, F., Baldwin, T., and Lau, J.~H. (2020).
\newblock Ffci: A framework for interpretable automatic evaluation of
  summarization.

\bibitem[Krippendorff, 1970]{krippendorff1970}
Krippendorff, K. (1970).
\newblock Estimating the reliability, systematic error and random error of
  interval data.
\newblock {\em Educational and Psychological Measurement}, 30(1):61--70.

\bibitem[Köpf et~al., 2023]{openassistant}
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens,
  K., Barhoum, A., Duc, N.~M., Stanley, O., Nagyfi, R., ES, S., Suri, S.,
  Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and
  Mattick, A. (2023).
\newblock Openassistant conversations -- democratizing large language model
  alignment.

\bibitem[Lazer et~al., 2018]{Lazer1094}
Lazer, D. M.~J., Baum, M.~A., Benkler, Y., Berinsky, A.~J., Greenhill, K.~M.,
  Menczer, F., Metzger, M.~J., Nyhan, B., Pennycook, G., Rothschild, D.,
  Schudson, M., Sloman, S.~A., Sunstein, C.~R., Thorson, E.~A., Watts, D.~J.,
  and Zittrain, J.~L. (2018).
\newblock The science of fake news.
\newblock {\em Science}, 359(6380):1094--1096.

\bibitem[Lehe{\v{c}}ka and {\v{S}}vec, 2021]{fernet}
Lehe{\v{c}}ka, J. and {\v{S}}vec, J. (2021).
\newblock Comparison of czech transformers on text classification tasks.
\newblock In Espinosa-Anke, L., Mart{\'i}n-Vide, C., and Spasi{\'{c}}, I.,
  editors, {\em Statistical Language and Speech Processing}, pages 27--37,
  Cham. Springer International Publishing.

\bibitem[Lin, 2004]{lin-2004-rouge}
Lin, C.-Y. (2004).
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In {\em Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain. Association for Computational Linguistics.

\bibitem[Liu et~al., 2022]{peft}
Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C.
  (2022).
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than
  in-context learning.

\bibitem[Liu et~al., 2023a]{prompting}
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023a).
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em ACM Comput. Surv.}, 55(9).

\bibitem[Liu et~al., 2023b]{Liu_2023}
Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He,
  M., Liu, Z., Wu, Z., Zhao, L., Zhu, D., Li, X., Qiang, N., Shen, D., Liu, T.,
  and Ge, B. (2023b).
\newblock Summary of {ChatGPT}-related research and perspective towards the
  future of large language models.
\newblock {\em Meta-Radiology}, 1(2):100017.

\bibitem[Liu et~al., 2019]{roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V. (2019).
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock {\em CoRR}, abs/1907.11692.

\bibitem[Mikolov et~al., 2013]{mikolov}
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).
\newblock Efficient estimation of word representations in vector space.

\bibitem[Mlynář, 2023]{mlynar}
Mlynář, T. (2023).
\newblock Automated fact checking based on czech wikipedia.
\newblock \url{https://dspace.cvut.cz/handle/10467/109219}.

\bibitem[Mohri et~al., 2023]{mohri2023learning}
Mohri, C., Andor, D., Choi, E., and Collins, M. (2023).
\newblock Learning to reject with a fixed predictor: Application to
  decontextualization.

\bibitem[Mroczkowski et~al., 2021]{mroczkowski-etal-2021-herbert}
Mroczkowski, R., Rybak, P., Wr{\'o}blewska, A., and Gawlik, I. (2021).
\newblock {H}er{BERT}: Efficiently pretrained transformer-based language model
  for {P}olish.
\newblock In {\em Proceedings of the 8th Workshop on Balto-Slavic Natural
  Language Processing}, pages 1--10, Kiyv, Ukraine. Association for
  Computational Linguistics.

\bibitem[Nakov et~al., 2021]{clef21}
Nakov, P., Martino, G. D.~S., Elsayed, T., Barrón-Cedeño, A., Míguez, R.,
  Shaar, S., Alam, F., Haouari, F., Hasanain, M., Mansour, W., Hamdan, B., Ali,
  Z.~S., Babulkov, N., Nikolov, A., Shahi, G.~K., Struß, J.~M., Mandl, T.,
  Kutlu, M., and Kartal, Y.~S. (2021).
\newblock Overview of the clef--2021 checkthat! lab on detecting check-worthy
  claims, previously fact-checked claims, and fake news.

\bibitem[Narayan et~al., 2018]{narayan-etal-2018-dont}
Narayan, S., Cohen, S.~B., and Lapata, M. (2018).
\newblock Don{'}t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807, Brussels, Belgium.
  Association for Computational Linguistics.

\bibitem[N{\o}rregaard and Derczynski, 2021]{norregaard2021danfever}
N{\o}rregaard, J. and Derczynski, L. (2021).
\newblock {D}an{FEVER}: claim verification dataset for {D}anish.
\newblock In {\em Proceedings of the 23rd Nordic Conference on Computational
  Linguistics (NoDaLiDa)}, pages 422--428, Reykjavik, Iceland (Online).
  Link{\"o}ping University Electronic Press, Sweden.

\bibitem[OpenAI, 2023]{gpt4}
OpenAI (2023).
\newblock Gpt-4 technical report.

\bibitem[Pan et~al., 2021]{pan2021zeroshot}
Pan, L., Chen, W., Xiong, W., Kan, M.-Y., and Wang, W.~Y. (2021).
\newblock Zero-shot fact verification by claim generation.

\bibitem[Patel and Ahmad, 2023]{moat}
Patel, D. and Ahmad, A. (2023).
\newblock Google "we have no moat, and neither does openai".
\newblock
  \url{https://www.semianalysis.com/p/google-we-have-no-moat-and-neither}.
\newblock Accessed: 2023-09-06.

\bibitem[Pennington et~al., 2014]{pennington-etal-2014-glove}
Pennington, J., Socher, R., and Manning, C. (2014).
\newblock {G}lo{V}e: Global vectors for word representation.
\newblock In {\em Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pages 1532--1543, Doha, Qatar.
  Association for Computational Linguistics.

\bibitem[Pikuliak et~al., 2021]{pikuliak2021slovakbert}
Pikuliak, M., Štefan Grivalský, Konôpka, M., Blšták, M., Tamajka, M.,
  Bachratý, V., Šimko, M., Balážik, P., Trnka, M., and Uhlárik, F. (2021).
\newblock Slovakbert: Slovak masked language model.

\bibitem[Pomerlau and Rao, 2017]{fncweb}
Pomerlau, D. and Rao, D. (2017).
\newblock Fake news challenge: Exploring how artificial intelligence
  technologies could be leveraged to combat fake news.
\newblock \url{http://www.fakenewschallenge.org}.
\newblock Accessed: 2023-09-06.

\bibitem[Raffel et~al., 2019]{t5-11b}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J. (2019).
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em CoRR}, abs/1910.10683.

\bibitem[Raffel et~al., 2020]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J. (2020).
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.

\bibitem[Rýpar, 2021]{rypar}
Rýpar, M. (2021).
\newblock Methods of document retrieval for fact-checking.
\newblock \url{https://www.overleaf.com/read/thbvcjvvvfjp}.
\newblock [Online; accessed 21-May-2021].

\bibitem[Schuster et~al., 2021]{DBLP:journals/corr/abs-2103-08541}
Schuster, T., Fisch, A., and Barzilay, R. (2021).
\newblock Get your vitamin c! robust fact verification with contrastive
  evidence.
\newblock {\em CoRR}, abs/2103.08541.

\bibitem[Sebastian, 2023]{glorin}
Sebastian, G. (2023).
\newblock Exploring ethical implications of chatgpt and other ai chatbots and
  regulation of disinformation propagation.

\bibitem[Sido et~al., 2021]{czert}
Sido, J., Pražák, O., Přibáň, P., Pašek, J., Seják, M., and Konopík, M.
  (2021).
\newblock Czert -- czech bert-like model for language representation.

\bibitem[{\textsf{STEM}}, 2021]{stem}
{\textsf{STEM}} (2021).
\newblock Mýtům a konspiracím o covid-19 věří více než třetina české
  internetové populace | stem.cz.
\newblock
  \url{https://www.stem.cz/mytum-a-konspiracim-o-covid-19-veri-vice-nez-tretina-ceske-internetove-populace/}.
\newblock Accessed: 2021-05-03.

\bibitem[Straka et~al., 2021]{straka2021robeczech}
Straka, M., Náplava, J., Straková, J., and Samuel, D. (2021).
\newblock {RobeCzech}: Czech {RoBERTa}, a monolingual contextualized language
  representation model.
\newblock {\em Lecture Notes in Computer Science}, page 197–209.

\bibitem[Stănescu, 2022]{georgiana_stanescu_2022_6795674}
Stănescu, G. (2022).
\newblock {Ukraine conflict: the challenge of informational war}.
\newblock {\em SOCIAL SCIENCES AND EDUCATION RESEARCH REVIEW}, 9(1):146--148.

\bibitem[Taori et~al., 2023]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,
  P., and Hashimoto, T.~B. (2023).
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \url{https://crfm.stanford.edu/2023/03/13/alpaca.html}.
\newblock Accessed: 2023-09-04.

\bibitem[\textsf{NLP-Progress}, 2023]{nlpprogress}
\textsf{NLP-Progress} (2023).
\newblock On summarization.
\newblock \url{http://nlpprogress.com/english/summarization.html}.
\newblock Accessed: 2023-09-06.

\bibitem[Thorne et~al., 2018a]{fever}
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. (2018a).
\newblock {FEVER}: a large-scale dataset for fact extraction and
  {VERification}.
\newblock In {\em NAACL-HLT}.

\bibitem[Thorne et~al., 2018b]{fever2018}
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. (2018b).
\newblock {FEVER}: a large-scale dataset for fact extraction and
  {VERification}.
\newblock In {\em NAACL-HLT}.

\bibitem[Thorne et~al., 2018c]{fever1}
Thorne, J., Vlachos, A., Cocarascu, O., Christodoulopoulos, C., and Mittal, A.
  (2018c).
\newblock The {Fact Extraction and VERification (FEVER)} shared task.
\newblock In {\em Proceedings of the First Workshop on {Fact Extraction and
  VERification (FEVER)}}.

\bibitem[Thorne et~al., 2019]{fever2}
Thorne, J., Vlachos, A., Cocarascu, O., Christodoulopoulos, C., and Mittal, A.
  (2019).
\newblock The {FEVER}2.0 shared task.
\newblock In {\em Proceedings of the Second Workshop on Fact Extraction and
  VERification (FEVER)}, pages 1--6, Hong Kong, China. Association for
  Computational Linguistics.

\bibitem[Touvron et~al., 2023a]{llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin,
  A., Grave, E., and Lample, G. (2023a).
\newblock Llama: Open and efficient foundation language models.

\bibitem[Touvron et~al., 2023b]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
  Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
  W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S.,
  Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
  A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
  Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y.,
  Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva,
  R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R.,
  Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A.,
  Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and
  Scialom, T. (2023b).
\newblock Llama 2: Open foundation and fine-tuned chat models.

\bibitem[Ullrich, 2021]{diplomka}
Ullrich, H. (2021).
\newblock Dataset for automated fact checking in czech language.
\newblock \url{https://dspace.cvut.cz/handle/10467/95430}.

\bibitem[Ullrich et~al., 2023]{lrev}
Ullrich, H., Drchal, J., R{\'y}par, M., Vincourov{\'a}, H., and Moravec, V.
  (2023).
\newblock Csfever and ctkfacts: acquiring czech data for fact verification.
\newblock {\em Language Resources and Evaluation}.

\bibitem[Vaswani et~al., 2017]{vaswani}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em CoRR}, abs/1706.03762.

\bibitem[Vicuna, 2023]{vicuna}
Vicuna (2023).
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock \url{https://vicuna.lmsys.org/}.
\newblock Accessed: 2023-09-04.

\bibitem[Wang et~al., 2020]{wang-etal-2020-asking}
Wang, A., Cho, K., and Lewis, M. (2020).
\newblock Asking and answering questions to evaluate the factual consistency of
  summaries.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 5008--5020, Online. Association for
  Computational Linguistics.

\bibitem[Wardle and Derakhshan, 2017]{disorder}
Wardle, C. and Derakhshan, H. (2017).
\newblock {\em INFORMATION DISORDER : Toward an interdisciplinary framework for
  research and policy making Information Disorder Toward an interdisciplinary
  framework for research and policymaking}.

\bibitem[Wright et~al., 2022]{wright}
Wright, D., Wadden, D., Lo, K., Kuehl, B., Cohan, A., Augenstein, I., and Wang,
  L.~L. (2022).
\newblock Generating scientific claims for zero-shot scientific fact checking.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2448--2460, Dublin,
  Ireland. Association for Computational Linguistics.

\bibitem[Yamada et~al., 2020]{yamada2020luke}
Yamada, I., Asai, A., Shindo, H., Takeda, H., and Matsumoto, Y. (2020).
\newblock Luke: Deep contextualized entity representations with entity-aware
  self-attention.

\bibitem[Yasunaga et~al., 2021]{yasunaga-etal-2021-lm}
Yasunaga, M., Leskovec, J., and Liang, P. (2021).
\newblock {LM}-critic: Language models for unsupervised grammatical error
  correction.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7752--7763, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[Zha et~al., 2023]{zha2023alignscore}
Zha, Y., Yang, Y., Li, R., and Hu, Z. (2023).
\newblock Alignscore: Evaluating factual consistency with a unified alignment
  function.

\bibitem[Zhang* et~al., 2020]{bert-score}
Zhang*, T., Kishore*, V., Wu*, F., Weinberger, K.~Q., and Artzi, Y. (2020).
\newblock Bertscore: Evaluating text generation with bert.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zhao et~al., 2023]{llms}
Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,
  Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren,
  R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. (2023).
\newblock A survey of large language models.

\end{thebibliography}

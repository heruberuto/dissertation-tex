%!TEX ROOT=../dissertation.tex

\chapter{Dissertation plan}
\label{chap:plan}
	extit{This chapter describes my current research agenda for automated fact‑checking with NLP methods; the final section outlines the dissertation.}

\section{Automated claim generation}
\label{generation}

An article in preparation proposes \textit{automated claim generation} as extracting factual claims from a document. This can assist fact‑checkers and emulate data for related tasks (fact‑checking, NLI).

Extracting fluent, atomic claims from naturally written text raises challenges—what information best characterizes the text? 
How does one resolve the pronouns and coreferences in source text?
How does one adapt the extraction scheme for different speakers and stylistic forms?

These problems overlap with \textit{abstractive summarization}, which recently made progress via Transformer models~\cite{pegasus,brio}.

The summarization setup needs minor tweaks: enforce single‑sentence outputs and encourage diverse factual foci via sampling (top‑$k$\footnote{Each output token is sampled from the $k$ most probable words.} and top‑$p$\footnote{Each token is sampled from the smallest prefix of tokens whose total probability mass is at most $p$.}~\cite{topp}).

Initial training data comes from XL‑Sum~\cite{xlsum}, \FEN, and \CTK. We train mBART, Pegasus, T5~\cite{t5-11b}, and LLaMA‑2 (QLoRA) models, with additional experiments via chat APIs.

Going forward, we will iterate on data and models in tandem and, crucially, develop reliable, explainable metrics that correlate with human judgment.
As shown in section~\ref{benchmarking-sota}, the standard automated summarization metrics are not appropriate as a benchmark for the task.

\section{Claim generation metrics}
\label{metrics}

\begin{figure}
    \includegraphics[width=7cm]{fig/lm_critics.png}
    \caption{\textsf{LM-Critic} -- deciding text fluency viewed as finding local optima of Language Model output probability, reprinted from~\cite{yasunaga-etal-2021-lm}}
    \label{fig:lmcritic}
\end{figure}

\begin{figure}
    \includegraphics[width=11cm]{fig/gptext.pdf}
    \caption{A self-evaluating claim generation model based on GPT-3.5-turbo and GPT-4~\cite{gpt4} using the \textsf{OpenAI API} and a single-shot (one gold example given) approach}
    \label{fig:gptext}
\end{figure}
Generative tasks pose two issues: explaining model behavior in human‑understandable terms and diagnosing failures such as \textit{hallucinations}.

For the task of claim generation, where we also face the challenge of the \textit{relevance} of the information extracted by the model, we suggest the use of the following metrics rooted in the current research on similar topics:

\begin{enumerate}
    \item {\techbf Fluency} -- \textit{is the claim grammatically correct and intelligible?}
    
    We explore two proxies for claim fluency—akin to Grammatical Error Detection (GED): \textsf{LM‑Critic} (Figure~\ref{fig:lmcritic})~\cite{yasunaga-etal-2021-lm} perturbs tokens to probe local optima of LM probabilities (e.g., GPT‑2 as reference), and \textsf{GPTScore}~\cite{fu2023gptscore} prompts an LLM (e.g., GPT‑3) for a direct fluency score in few‑/zero‑shot settings.
    
    Both can be adapted for Czech and the latter is demonstrated in Figure~\ref{fig:gptext}.
    \item {\techbf Decontextualization} -- \textit{can the claim be correctly interpreted without any additional context from the source document or elsewhere?}
    
    A common problem with machine-extracted factual claims is reusing excerpts from source documents along with inexplicable contextual pronouns (\"{President won't sue \textit{them}}) and relative referencing (\"{\textit{Last year}, CTU had 23K students}).

\cite{choi-etal-2021-decontextualization} proposes decontextualization as a sequence-to-sequence task with two texts on input $(s,c)$ -- sentence and context.
    T5 model~\cite{t5-11b} is then trained on machine-generated gold data from Wikipedia to output sentence $s^\prime$ such that the truth-conditional meaning of $s^\prime$ in an empty context is the same as that of $s$ in $c$.

    \cite{mohri2023learning} improves upon this, altering the problem formulation to minimization of surrogate loss, rejecting with a fixed predictor, and claiming to get as close as $\sim3\%$ away from the theoretical limit for the task.

    The approaches are reproducible using the Czech Wikipedia corpus and appropriate for further examination.

    \item {\techbf Atomicity} --  \textit{does the claim describe a single entity, relation or process?}
    \label{atomicity}
    
    Claim atomicity can be proxied via Relation Extraction (RE), e.g., LUKE~\cite{yamada2020luke}. Identify entities and relations (e.g., $(\textit{study at}, \textit{Herbert}, \textit{CTU})$); mark a claim as atomic if at most one such triple is found (after symmetry normalization).

    \item {\techbf Faithfulness} -- \textit{does the claim only contain information that is consistent with the source document?}
    
    This metric is crucial to pinpoint \textit{hallucinations}—parts of the claim unsupported by the source. We use two alternatives: a score within the FFCI framework~\cite{ffci}:
    $$\text{AvgTop-}n_{s_j\in X, t_i\in Y^\prime}(\textsc{BERTScore}(t_i,s_j))$$
    Here $\text{AvgTop-}n$ averages the top $n$ (e.g., $n{=}5$) scores; $X$ and $Y^\prime$ are sentence sets from the source and the model output (for claim generation, $|Y^\prime|{=}1$). $\textsc{BERTScore}$~\cite{bert-score} compares sentence embeddings rather than surface word overlap (e.g., ROUGE~\cite{lin-2004-rouge}), which helps in morphologically rich languages like Czech.

    A related metric, \textsc{AlignScore}~\cite{zha2023alignscore}, optimizes alignment between input and output spans via a RoBERTa~\cite{roberta} model trained for inconsistency detection on 4.7M examples spanning NLI, QA, paraphrase, etc. Despite being relatively small (355M parameters), it outperforms some GPT‑4‑based metrics.

    Empirically, the models work encouragingly well on spotting hallucinations and inconsistencies in English, and while the transduction of \textsc{BERTScore} is trivial, using a Czech embedding model such as CZERT~\cite{czert} or FERNET~\cite{fernet}, reproducing the success of \textsc{AlignScore} will require more research and data.

    \item {\techbf Focus}$@k$ -- \textit{if we generate $k$ claims using this model, what will be the proportion of gold (relevant) information among all the information listed in the generated claims?}
    
    The metric is analogous to \textit{precision}, but decisions are ambiguous in natural language due to synonyms and many valid phrasings.

    An elegant and functional perspective on the problem has been brought around in QAGS\footnote{Pronounced \"{kags}, stands for \"{Question Answering and Generation for Summarization}} evaluation protocol~\cite{wang-etal-2020-asking}, where the idea is to use a Question Generation model (QG) to formulate questions in natural language based on all $k$ predicted claims. The questions are then twice answered using a Question Answering (QA) model, giving it knowledge from (i.) the predicted claims (ii.) the gold claims written by a human.
    The focus is then defined as the proportion of questions with the same answers extracted from the gold and predicted claims among all questions the model can generate from the predicted claims. 

    \item {\techbf Coverage}$@k$ -- \textit{if we generate $k$ claims using this model, what proportion of gold (relevant) information from the source text will be covered?}
    
    Analogous to \textit{recall@k} in general machine learning, QAGS proposes to generate questions using gold claims and try to answer them using the predicted claims, much like in the \textit{focus} scenario, but vice versa.
\end{enumerate}
The metrics are proposed in accordance with other research on model-based evaluation of similar NLP tasks~\cite{ffci,wright} and are to be refined upon experiments with annotators.

\section{Data collection}
\subsection{Human-in-the-loop grading of claim generators}
To validate and progress the metrics referred to in section~\ref{metrics}, one needs human-annotated data for the task.
I aim to use an experiment similar to that of~\cite{wright}, presenting annotators with ordinal scales for the claim qualities and appropriate grading for each metric conditioned by objective rules.

My research will attempt to design the experiment in a way that yields the best data, checking its validity using inter-annotator agreement and other forms of feedback and publishing the data and scheme alongside the other solutions. 
Collected data will be used to validate the prototype metrics from section~\ref{metrics} and propose their variations based on the findings.

\subsection{Polish dataset scraping}
While Czech has its SumeCzech~\cite{straka-etal-2018-sumeczech} and in Slovak, we can still reproduce the SMESum~\cite{suppa-adamec-2020-summarization} research, a large-scale single-sentence summarization dataset in Polish has yet to be established.
The closest data I have found is the online news corpus~\cite{szwoch-etal-2022-creation} collected for the purposes of studying political polarization (and nowhere published, despite my e-mail urgences).

A scraping experiment in the Polish media, such as TVP, Rzeczpospolita, Gazeta Wyborcza, Fakt, etc., is therefore being prepared to obtain an appropriate single-sentence dataset for publication -- it is also going to be another incremental step toward the dissertation on the overall topic of NLP fact-checking and its stages, focusing on English and West Slavic languages. 

\subsection{Crowd-sourced fact-checking platform}
In 2023, other members of our team~\cite{butora} with funding from Avast developed a crowd-sourced fact-checking platform\footnote{\url{https://factcheck.fel.cvut.cz}}, where users gather reputations like on Wikipedia, by sharing check-worthy pieces of information found across the internet, and by their checking with sources.

While I am not directly involved in the implementation of the project apart from early consulting, experiments with FSV CUNI are to be launched, populating this platform with data and users. 
After the experiments, other data and applications will be delivered, and their processing will be another part of my dissertation project.

\subsection{\CTK expansion}
In 2021/2022, another round of the \CTK annotation experiment (see section~\ref{sec:ctkfacts}) was carried out with the FSV CUNI students, yielding about 5K new data points, including, for example, claims extracted from the Czech Twitter.

The data is being cleaned and examined and will be attached to one of the other upcoming publications and presented in the dissertation thesis.

\section{Pipeline modernization}
As mentioned throughout the chapter~\ref{chap:sota}, the state of the art in NLP has shifted dramatically over the last year, and another of the tasks I am currently working on is the modernization of our pipeline -- Claim Generation, Information Retrieval, Natural Language Inference models -- and appropriate use of LLMs in the tasks.

So far, I have successfully finetuned LLaMA-2~\cite{llama2} for the claim generation task, and we have a LoRA finetuning experimental setup ready for the NLI models.
The use of LLaMA-2 and its successors for our tasks will be a topic on its own, as most publicly available LLMs filter out the other languages and focus solely on English.

\section{The grand scope}
Overall, in brief points, the main topics of my dissertation are expected to be:
\begin{enumerate}
    \item Introduction of the fact-checking task and its data, strong model baselines, and specific properties in the \textbf{West Slavic} context.
    \item An integration of the step of \textbf{Claim generation} step into it, based on methods of abstractive summarization.
    \item A delivery of reliable \textbf{metrics} for the tasks and their validation with expert-level humans.
    \item Modernization of the automated fact-checking framework and solutions in English and Czech into the age of \textbf{Large Language Models}.
    Solutions were proposed already -- based on proprietary black-box LLMs such as GPT-3.5~\cite{bing} -- our next goal is to deploy open-source LLMs in-house, experiment with different architectures, fine-tuning tasks and data, improving the SOTA on our benchmark data.

    Due to our aim to produce transparent and reproducible research, using open-source LLMs is preferred over popular proprietary ones like GPT-4.
    \item As the current instruction-tuned Large Language Models exhibit an \textbf{ability to explain their reasoning}~\cite{SAEED2023110273}, the methods of eXplainable AI (XAI) may also be integrated into our automated fact-checking framework, giving the fact-checker further insights what is behind the model classification.
    \item Multiple \textbf{validation experiments} are planned with real-world fact-checkers\footnote{Partners from CEDMO and other projects} to testify to the usability of our solutions in the real world.
\end{enumerate}